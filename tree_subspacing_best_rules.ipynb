{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = {\n",
    "    'train': 'breast-train-0-s1.csv',\n",
    "    'test': 'breast-test-0-s1.csv'\n",
    "}\n",
    "SAVE_RESULTS_PATH = \"workdir/results/res1-new.results\"\n",
    "N_ESTIMATORS = 5\n",
    "MIN_SAMPLES_SPLIT = 2\n",
    "N_JOBS = 1\n",
    "MAX_DEPTH = 5\n",
    "SUBSPACES = 5\n",
    "CV = 5\n",
    "CV_REPEATS = 10\n",
    "RF_TYPE = 'randomForest' # 'randomForest'\n",
    "SELECTION_METHODS = ['balanced_accuracy', 'accuracy', 'rf_accuracy', 'rf_balanced_accuracy', 'accuracy/accuracy_stddev'] # 'balanced_accuracy' # 'accuracy', 'f1_weighted'\n",
    "PARALLELISM = \"loky\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "import joblib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from joblib import Parallel, delayed\n",
    "from networkx.algorithms.clique import find_cliques\n",
    "from rules.api import AdjacentOrNot\n",
    "from rules.classification.rule_measures import BayesianRuleMeasures, covered_by_statements\n",
    "from rules.classification.subspace_rules_classifier import SubspaceRulesClassifier\n",
    "from rules.note.extract_rules import extract_rules\n",
    "from rules.note.overlapping.measure_adjacencies import measure_rules\n",
    "from rules.utils.utils import join_consecutive_statements\n",
    "from scipy.stats import entropy\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score, make_scorer, confusion_matrix, f1_score, precision_score, balanced_accuracy_score, cohen_kappa_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sympy import parse_expr\n",
    "from toolz.curried import pipe, filter, map, reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATASET_PATH['train'])\n",
    "test_data = pd.read_csv(DATASET_PATH['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.drop('TARGET', axis=1).values\n",
    "y_train = train_data['TARGET'].values\n",
    "x_test = test_data.drop('TARGET', axis=1).values\n",
    "y_test = test_data['TARGET'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "        **DATASET_PATH,\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RF_TYPE == 'randomForest':\n",
    "    clf_rf = RandomForestClassifier(max_depth=MAX_DEPTH, n_estimators=N_ESTIMATORS, min_samples_split=MIN_SAMPLES_SPLIT, random_state=42)\n",
    "elif RF_TYPE == 'extraTrees':\n",
    "    clf_rf = ExtraTreesClassifier(max_depth=MAX_DEPTH, n_estimators=N_ESTIMATORS, min_samples_split=MIN_SAMPLES_SPLIT, random_state=42)\n",
    "clf_rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt = DecisionTreeClassifier(random_state=42, min_samples_split=MIN_SAMPLES_SPLIT, max_depth=MAX_DEPTH)\n",
    "clf_dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_predict = clf_rf.predict(x_test)\n",
    "dt_test_predict = clf_dt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "    **meta,\n",
    "    \"DT_test_acc\": accuracy_score(y_test, dt_test_predict),\n",
    "    \"RF_test_acc\": accuracy_score(y_test, rf_test_predict),\n",
    "    \"DT_test_confusion_matrix\": confusion_matrix(y_test, dt_test_predict),\n",
    "    \"RF_test_confusion_matrix\": confusion_matrix(y_test, rf_test_predict),\n",
    "    \"RF_test_f1\": f1_score(y_test, rf_test_predict, average='weighted'),\n",
    "    \"DT_test_f1\": f1_score(y_test, dt_test_predict, average='weighted'),\n",
    "    \"DT_test_gmean\": geometric_mean_score(y_test, dt_test_predict, average='weighted'),\n",
    "    \"RF_test_gmean\": geometric_mean_score(y_test, rf_test_predict, average='weighted'),\n",
    "    \"DT_test_balanced_accuracy\": balanced_accuracy_score(y_test, dt_test_predict),\n",
    "    \"RF_test_balanced_accuracy\": balanced_accuracy_score(y_test, rf_test_predict),\n",
    "    \"x_test\": x_test,\n",
    "    \"RF_test_predictions\": rf_test_predict,\n",
    "    \"DT_test_predictions\": dt_test_predict,\n",
    "    \"SELECTION_METHODS\": SELECTION_METHODS,\n",
    "    \"CV\": CV,\n",
    "    \"RF_TYPE\": RF_TYPE,\n",
    "    \"CV_REPEATS\": CV_REPEATS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_rules = pipe(\n",
    "    clf_rf.estimators_,\n",
    "    map(lambda estimator: extract_rules(estimator)),\n",
    "    reduce(set.union),\n",
    "    map(lambda r: join_consecutive_statements(r)),\n",
    "#     map(lambda r: bound_rule(r, x_train)),\n",
    "    list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_statements = pipe(\n",
    "    all_rules,\n",
    "    set,\n",
    "    map(lambda r: list(r.statements)),\n",
    "    reduce(list.__add__),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all non-overlapping rule cliques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with joblib.parallel_backend('threading'):\n",
    "    all_rule_measurements= measure_rules(all_rules, n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_graph(measurements_tuple, measurement):\n",
    "    \n",
    "    if measurement == AdjacentOrNot.NOT_ADJACENT:\n",
    "        rule_1, rule_2 = measurements_tuple\n",
    "        rule_idx_1 = all_rules.index(rule_1)\n",
    "        rule_idx_2 = all_rules.index(rule_2)\n",
    "        \n",
    "        g.add_node(rule_idx_1)\n",
    "\n",
    "        g.add_node(rule_idx_2)\n",
    "\n",
    "        g.add_edge(rule_idx_1, rule_idx_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for measurements_tuple, measurement in all_rule_measurements.items():\n",
    "    \n",
    "    add_to_graph(measurements_tuple, measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_subspaces = list(filter(lambda s: len(s) <= SUBSPACES)(find_cliques(g)))\n",
    "\n",
    "subspaces_to_check = all_subspaces\n",
    "\n",
    "for clique_size in reversed(range(1, SUBSPACES + 1)):\n",
    "    subspaces_by_size_of_param = list(filter(lambda s: len(s) == SUBSPACES)(all_subspaces))\n",
    "    print(f\"clique = {clique_size}\")\n",
    "    if not len(subspaces_by_size_of_param) == 0:\n",
    "        subspaces_to_check = subspaces_by_size_of_param\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not subspaces_to_check:\n",
    "    subspaces_to_check = [[r_idx] for r_idx in list(range(len(all_rules)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subspaces_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "    **meta,\n",
    "    'total_rules' : len(all_rules),\n",
    "    'total_subspaces_to_check': len(subspaces_to_check),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_count = len(np.unique(y_train))\n",
    "\n",
    "def calculate_entropy(y, classes_count):\n",
    "    counts = np.unique(y, return_counts=True)[1]\n",
    "    return entropy(counts, base=classes_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_with_rf(estimator, X, y):\n",
    "    y_rf = clf_rf.predict(X)\n",
    "    y_model = estimator.predict(X)\n",
    "    \n",
    "    return accuracy_score(y_rf, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bal_accuracy_with_rf(estimator, X, y):\n",
    "    y_rf = clf_rf.predict(X)\n",
    "    y_model = estimator.predict(X)\n",
    "    \n",
    "    return balanced_accuracy_score(y_rf, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_kohen_cappa(estimator, X, y):\n",
    "    y_rf = clf_rf.predict(X)\n",
    "    y_model = estimator.predict(X)\n",
    "    \n",
    "    return cohen_kappa_score(y_rf, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(clique, x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    rules = np.array(all_rules)[clique]\n",
    "    clf = SubspaceRulesClassifier(rules=rules, max_depth=MAX_DEPTH, random_state=42)\n",
    "\n",
    "    skf = ShuffleSplit(n_splits=CV, test_size=0.5,random_state=42)\n",
    "    with joblib.parallel_backend('threading'):\n",
    "        scores = cross_validate(clf, x_train, y_train, n_jobs=1, scoring={\n",
    "            'balanced_accuracy': 'balanced_accuracy',\n",
    "            'f1': 'f1_weighted',\n",
    "            'accuracy': 'accuracy',\n",
    "            'g_mean': make_scorer(geometric_mean_score, average='weighted'),\n",
    "            'recall': 'recall_weighted',\n",
    "            'precision': 'precision_weighted',\n",
    "            'rf_accuracy': accuracy_with_rf,\n",
    "            'rf_balanced_accuracy': bal_accuracy_with_rf,\n",
    "            'rf_cohen_kappa': rf_kohen_cappa\n",
    "        }, cv=skf)\n",
    "\n",
    "    additional_scores = defaultdict(list)\n",
    "\n",
    "    skf = ShuffleSplit(n_splits=CV, test_size=0.5, random_state=42)\n",
    "    for train_index, test_index in skf.split(x_train, y_train):\n",
    "        x_train_split = x_train[train_index]\n",
    "        y_train_split = y_train[train_index]\n",
    "\n",
    "        mean_rule_scores = defaultdict(list)\n",
    "        for rule in rules:\n",
    "            covered_indicies = list(covered_by_statements(rule, x_train_split))\n",
    "\n",
    "            rule_measurements = BayesianRuleMeasures.create(rule, x_train_split, y_train_split)\n",
    "            mean_rule_scores['a'].append(rule_measurements.a())\n",
    "            mean_rule_scores['b'].append(rule_measurements.b())\n",
    "            mean_rule_scores['c'].append(rule_measurements.c())\n",
    "            mean_rule_scores['d'].append(rule_measurements.d())\n",
    "            mean_rule_scores['s'].append(rule_measurements.s_measure())\n",
    "            mean_rule_scores['n'].append(rule_measurements.n_measure())\n",
    "            mean_rule_scores['entropy'].append(calculate_entropy(y_train_split[covered_indicies], classes_count))\n",
    "\n",
    "        for score_name, this_score in dict(mean_rule_scores).items():\n",
    "            additional_scores[score_name].append(np.mean(this_score))\n",
    "\n",
    "\n",
    "            \n",
    "    final_clf = SubspaceRulesClassifier(rules=rules, max_depth=MAX_DEPTH, random_state=42)\n",
    "    final_clf.fit(x_train, y_train)\n",
    "    final_clf_y_pred = final_clf.predict(x_test)\n",
    "    final_model_test_accuracy = accuracy_score(y_test, final_clf_y_pred) \n",
    "    final_model_confusion_matrix = confusion_matrix(y_test, final_clf_y_pred)\n",
    "    \n",
    "    scores = {\n",
    "        **scores,\n",
    "        **dict(additional_scores),\n",
    "    }\n",
    "    scores_without_test_preffix = {\n",
    "        **{k[5:] if k.startswith('test_') else k: np.mean(v) for k, v in scores.items()},\n",
    "        **{f\"{k[5:]}_stddev\" if k.startswith('test_') else f\"{k}_stddev\": np.std(v) for k, v in scores.items()},\n",
    "    }\n",
    "    \n",
    "    score_by_selection_method = {\n",
    "        'score ' + method: float(parse_expr(method).evalf(subs=scores_without_test_preffix)) for method in SELECTION_METHODS\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        **scores_without_test_preffix, \n",
    "        **score_by_selection_method,\n",
    "        'final_model_test_accuracy': final_model_test_accuracy,\n",
    "        'final_model_test_predictions': final_clf_y_pred,\n",
    "        'final_model_confusion_matrix': final_model_confusion_matrix,\n",
    "        'final_model_used_trees': len(final_clf._clf_by_rule),\n",
    "        'final_model_test_f1_score': f1_score(y_test, final_clf_y_pred, average='weighted'),\n",
    "        'final_model_test_g_mean': geometric_mean_score(y_test, final_clf_y_pred, average='weighted'),\n",
    "        'final_model_test_recall_score': recall_score(y_test, final_clf_y_pred, average='weighted'),\n",
    "        'final_model_test_precision_score': precision_score(y_test, final_clf_y_pred, average='weighted'),\n",
    "        'final_model_test_balanced_accuracy_score': balanced_accuracy_score(y_test, final_clf_y_pred),\n",
    "        'final_model_original_rf_fidelity': accuracy_score(rf_test_predict, final_clf_y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with joblib.parallel_backend('threading'):\n",
    "#     with tqdm_joblib(tqdm(desc=\"My calculation\", total=len(subspaces_to_check))) as progress_bar:\n",
    "    score_by_subspace = \\\n",
    "        dict(zip(\n",
    "            map(tuple)(subspaces_to_check), \n",
    "            Parallel(n_jobs=N_JOBS)(delayed(lambda subspace: get_val(subspace, x_train, y_train, x_test, y_test))(subspace) for subspace in subspaces_to_check)\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_end = time.time()\n",
    "scoring_time = scoring_end - scoring_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rules = max(score_by_subspace, key=lambda s: score_by_subspace[s]['final_model_test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = score_by_subspace[top_rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_selection_method = {}\n",
    "for selection_method in SELECTION_METHODS:\n",
    "    best_score = max([s[f'score {selection_method}'] for s in score_by_subspace.values()])\n",
    "    best_score_rules = [rules for rules, val in score_by_subspace.items() if val[f'score {selection_method}'] == best_score]\n",
    "    \n",
    "    best_score_rules_with_scoring = {\n",
    "        rules: score_by_subspace[rules] for rules in best_score_rules\n",
    "    } \n",
    "    worst = best_score_rules_with_scoring[min(best_score_rules_with_scoring, key=lambda v: best_score_rules_with_scoring[v]['accuracy'])]\n",
    "    best = best_score_rules_with_scoring[max(best_score_rules_with_scoring, key=lambda v: best_score_rules_with_scoring[v]['accuracy'])]\n",
    "    \n",
    "    scores_by_selection_method[selection_method] = {\n",
    "        **{f'worst_{k}': v for k, v in worst.items()},\n",
    "        **{f'best_{k}': v for k, v in worst.items()},\n",
    "        'found': len(best_score_rules)\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    **meta,\n",
    "    'scoring_time': scoring_time,\n",
    "    **scores_by_selection_method,\n",
    "    **{f'top_{k}': v for k, v in top.items()},\n",
    "    'found_rules': len(best_score_rules_with_scoring),\n",
    "    'all_subspaces_with_score': score_by_subspace\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
