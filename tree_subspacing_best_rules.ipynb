{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = {\n",
    "    'train': 'workdir/datasets/iris-train-2-s1.csv', \n",
    "    'test': 'workdir/datasets/iris-test-2-s1.csv'\n",
    "}\n",
    "SAVE_RESULTS_PATH = \"workdir/results/res1-new.results\"\n",
    "N_ESTIMATORS = 5\n",
    "MIN_SAMPLES_SPLIT = 2\n",
    "N_JOBS = 1\n",
    "MAX_DEPTH = 5\n",
    "SUBSPACES = 5\n",
    "CV = 5\n",
    "CV_REPEATS = 10\n",
    "RF_TYPE = 'extraTrees' # 'randomForest'\n",
    "SELECTION_METHODS = ['balanced_accuracy', 'accuracy', 'rf_accuracy', 'rf_balanced_accuracy', 'accuracy/accuracy_stddev'] # 'balanced_accuracy' # 'accuracy', 'f1_weighted'\n",
    "PARALLELISM = \"loky\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from reduction.api import Instance, AdjacentOrNot, Relation, Rule, Features, Statement\n",
    "from reduction.extract_rules import extract_rules\n",
    "from reduction.measure_adjacencies import measure_rules\n",
    "from toolz.curried import pipe, filter, map, reduce\n",
    "from reduction.utils import bound_rule, join_consecutive_statements\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import recall_score, make_scorer, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, make_scorer, confusion_matrix\n",
    "from imblearn.metrics import geometric_mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import joblib\n",
    "\n",
    "if PARALLELISM == 'dask':\n",
    "    parallel_backend = 'dask'\n",
    "    client = Client(\"localhost:56436\")\n",
    "else:\n",
    "    parallel_backend = 'loky'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATASET_PATH['train'])\n",
    "test_data = pd.read_csv(DATASET_PATH['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.drop('TARGET', axis=1).values\n",
    "y_train = train_data['TARGET'].values\n",
    "x_test = test_data.drop('TARGET', axis=1).values\n",
    "y_test = test_data['TARGET'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "        **DATASET_PATH,\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from reduction.classification import measure_acc, to_instance, RulesClassifier, measure_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(max_depth=5, n_estimators=5, random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if RF_TYPE == 'randomForest':\n",
    "    clf_rf = RandomForestClassifier(max_depth=MAX_DEPTH, n_estimators=N_ESTIMATORS, min_samples_split=MIN_SAMPLES_SPLIT, random_state=42)\n",
    "elif RF_TYPE == 'extraTrees':\n",
    "    clf_rf = ExtraTreesClassifier(max_depth=MAX_DEPTH, n_estimators=N_ESTIMATORS, min_samples_split=MIN_SAMPLES_SPLIT, random_state=42)\n",
    "clf_rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5, random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_dt = DecisionTreeClassifier(random_state=42, min_samples_split=MIN_SAMPLES_SPLIT, max_depth=MAX_DEPTH)\n",
    "clf_dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_predict = clf_rf.predict(x_test)\n",
    "dt_test_predict = clf_dt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "    **meta,\n",
    "    \"DT_test_acc\": accuracy_score(y_test, dt_test_predict),\n",
    "    \"RF_test_acc\": accuracy_score(y_test, rf_test_predict),\n",
    "    \"DT_test_confusion_matrix\": confusion_matrix(y_test, dt_test_predict),\n",
    "    \"RF_test_confusion_matrix\": confusion_matrix(y_test, rf_test_predict),\n",
    "    \"RF_test_f1\": f1_score(y_test, rf_test_predict, average='weighted'),\n",
    "    \"DT_test_f1\": f1_score(y_test, dt_test_predict, average='weighted'),\n",
    "    \"DT_test_gmean\": geometric_mean_score(y_test, dt_test_predict, average='weighted'),\n",
    "    \"RF_test_gmean\": geometric_mean_score(y_test, rf_test_predict, average='weighted'),\n",
    "    \"DT_test_balanced_accuracy\": balanced_accuracy_score(y_test, dt_test_predict),\n",
    "    \"RF_test_balanced_accuracy\": balanced_accuracy_score(y_test, rf_test_predict),\n",
    "    \"x_test\": x_test,\n",
    "    \"RF_test_predictions\": rf_test_predict,\n",
    "    \"DT_test_predictions\": dt_test_predict,\n",
    "    \"SELECTION_METHODS\": SELECTION_METHODS,\n",
    "    \"CV\": CV,\n",
    "    \"RF_TYPE\": RF_TYPE,\n",
    "    \"CV_REPEATS\": CV_REPEATS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.1 ms, sys: 0 ns, total: 39.1 ms\n",
      "Wall time: 37.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_rules = pipe(\n",
    "    clf_rf.estimators_,\n",
    "    map(lambda estimator: extract_rules(estimator)),\n",
    "    reduce(set.union),\n",
    "    map(lambda r: join_consecutive_statements(r)),\n",
    "#     map(lambda r: bound_rule(r, x_train)),\n",
    "    list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_statements = pipe(\n",
    "    all_rules,\n",
    "    set,\n",
    "    map(lambda r: list(r.statements)),\n",
    "    reduce(list.__add__),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all non-overlapping rule cliques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.clique import enumerate_all_cliques, find_cliques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69 ms, sys: 1.3 ms, total: 70.3 ms\n",
      "Wall time: 68.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with joblib.parallel_backend(parallel_backend):\n",
    "    all_rule_measurements = measure_rules(all_rules, n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_graph(measurements_tuple, measurement):\n",
    "    \n",
    "    if measurement == AdjacentOrNot.NOT_ADJACENT:\n",
    "        rule_1, rule_2 = measurements_tuple\n",
    "        rule_idx_1 = all_rules.index(rule_1)\n",
    "        rule_idx_2 = all_rules.index(rule_2)\n",
    "        \n",
    "        g.add_node(rule_idx_1)\n",
    "\n",
    "        g.add_node(rule_idx_2)\n",
    "\n",
    "        g.add_edge(rule_idx_1, rule_idx_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.3 ms, sys: 904 Âµs, total: 36.2 ms\n",
      "Wall time: 34.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for measurements_tuple, measurement in all_rule_measurements.items():\n",
    "    \n",
    "    add_to_graph(measurements_tuple, measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clique = 5\n",
      "CPU times: user 4.6 ms, sys: 0 ns, total: 4.6 ms\n",
      "Wall time: 4.47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_subspaces = list(filter(lambda s: len(s) <= SUBSPACES)(find_cliques(g)))\n",
    "\n",
    "subspaces_to_check = all_subspaces\n",
    "\n",
    "for clique_size in reversed(range(1, SUBSPACES + 1)):\n",
    "    subspaces_by_size_of_param = list(filter(lambda s: len(s) == SUBSPACES)(all_subspaces))\n",
    "    print(f\"clique = {clique_size}\")\n",
    "    if not len(subspaces_by_size_of_param) == 0:\n",
    "        subspaces_to_check = subspaces_by_size_of_param\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not subspaces_to_check:\n",
    "    subspaces_to_check = [[r_idx] for r_idx in list(range(len(all_rules)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subspaces_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "    **meta,\n",
    "    'total_rules' : len(all_rules),\n",
    "    'total_subspaces_to_check': len(subspaces_to_check),\n",
    "#     'clique_size': clique_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 'workdir/datasets/iris-train-2-s1.csv',\n",
       " 'test': 'workdir/datasets/iris-test-2-s1.csv',\n",
       " 'DT_test_acc': 0.94,\n",
       " 'RF_test_acc': 0.96,\n",
       " 'DT_test_confusion_matrix': array([[17,  0,  0],\n",
       "        [ 0, 15,  2],\n",
       "        [ 0,  1, 15]]),\n",
       " 'RF_test_confusion_matrix': array([[17,  0,  0],\n",
       "        [ 0, 15,  2],\n",
       "        [ 0,  0, 16]]),\n",
       " 'RF_test_f1': 0.9599264705882353,\n",
       " 'DT_test_f1': 0.94,\n",
       " 'DT_test_gmean': 0.9553120086485319,\n",
       " 'RF_test_gmean': 0.970530479565019,\n",
       " 'DT_test_balanced_accuracy': 0.9399509803921569,\n",
       " 'RF_test_balanced_accuracy': 0.9607843137254902,\n",
       " 'x_test': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [5.9, 3. , 5.1, 1.8],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.9, 3.2, 5.7, 2.3]]),\n",
       " 'RF_test_predictions': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 0, 0, 1, 1, 2]),\n",
       " 'DT_test_predictions': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
       "        2, 0, 0, 1, 1, 2]),\n",
       " 'SELECTION_METHODS': ['balanced_accuracy',\n",
       "  'accuracy',\n",
       "  'rf_accuracy',\n",
       "  'rf_balanced_accuracy',\n",
       "  'accuracy/accuracy_stddev'],\n",
       " 'CV': 5,\n",
       " 'RF_TYPE': 'extraTrees',\n",
       " 'CV_REPEATS': 10,\n",
       " 'total_rules': 50,\n",
       " 'total_subspaces_to_check': 24}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from reduction.classification import SubspaceRulesClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, ShuffleSplit, RepeatedKFold, RepeatedStratifiedKFold, StratifiedShuffleSplit\n",
    "from imblearn.metrics import geometric_mean_score, sensitivity_score, specificity_score\n",
    "from sklearn.metrics import recall_score, make_scorer, confusion_matrix, f1_score, precision_score, balanced_accuracy_score, cohen_kappa_score\n",
    "from reduction.rule_measures import BayesianRuleMeasures, covered_by_statements\n",
    "from sympy import parse_expr, evalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "classes_count = len(np.unique(y_train))\n",
    "\n",
    "def calculate_entropy(y, classes_count):\n",
    "    counts = np.unique(y, return_counts=True)[1]\n",
    "    return entropy(counts, base=classes_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_with_rf(estimator, X, y):\n",
    "    y_rf = clf_rf.predict(X)\n",
    "    y_model = estimator.predict(X)\n",
    "    \n",
    "    return accuracy_score(y_rf, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bal_accuracy_with_rf(estimator, X, y):\n",
    "    y_rf = clf_rf.predict(X)\n",
    "    y_model = estimator.predict(X)\n",
    "    \n",
    "    return balanced_accuracy_score(y_rf, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_kohen_cappa(estimator, X, y):\n",
    "    y_rf = clf_rf.predict(X)\n",
    "    y_model = estimator.predict(X)\n",
    "    \n",
    "    return cohen_kappa_score(y_rf, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(clique, x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    rules = np.array(all_rules)[clique]\n",
    "    clf = SubspaceRulesClassifier(rules=rules, max_depth=MAX_DEPTH, random_state=42)\n",
    "    \n",
    "    all_scores = defaultdict(list)\n",
    "    all_additional_scores = defaultdict(list)\n",
    "    skf = ShuffleSplit(n_splits=CV, test_size=0.5,random_state=42)\n",
    "    with joblib.parallel_backend('threading'):\n",
    "        scores = cross_validate(clf, x_train, y_train, n_jobs=1, scoring={\n",
    "            'balanced_accuracy': 'balanced_accuracy',\n",
    "            'f1': 'f1_weighted',\n",
    "            'accuracy': 'accuracy',\n",
    "            'g_mean': make_scorer(geometric_mean_score, average='weighted'),\n",
    "            'recall': 'recall_weighted',\n",
    "            'precision': 'precision_weighted',\n",
    "            'rf_accuracy': accuracy_with_rf,\n",
    "            'rf_balanced_accuracy': bal_accuracy_with_rf,\n",
    "            'rf_cohen_kappa': rf_kohen_cappa\n",
    "        }, cv=skf)\n",
    "\n",
    "    additional_scores = defaultdict(list)\n",
    "\n",
    "    skf = ShuffleSplit(n_splits=CV, test_size=0.5, random_state=42)\n",
    "    for train_index, test_index in skf.split(x_train, y_train):\n",
    "        x_train_split = x_train[train_index]\n",
    "        y_train_split = y_train[train_index]\n",
    "\n",
    "        mean_rule_scores = defaultdict(list)\n",
    "        for rule in rules:\n",
    "            covered_indicies = list(covered_by_statements(rule, x_train_split))\n",
    "\n",
    "            rule_measurements = BayesianRuleMeasures.create(rule, x_train_split, y_train_split)\n",
    "            mean_rule_scores['a'].append(rule_measurements.a())\n",
    "            mean_rule_scores['b'].append(rule_measurements.b())\n",
    "            mean_rule_scores['c'].append(rule_measurements.c())\n",
    "            mean_rule_scores['d'].append(rule_measurements.d())\n",
    "            mean_rule_scores['s'].append(rule_measurements.s_measure())\n",
    "            mean_rule_scores['n'].append(rule_measurements.n_measure())\n",
    "            mean_rule_scores['entropy'].append(calculate_entropy(y_train_split[covered_indicies], classes_count))\n",
    "\n",
    "        for score_name, this_score in dict(mean_rule_scores).items():\n",
    "            additional_scores[score_name].append(np.mean(this_score))\n",
    "\n",
    "\n",
    "            \n",
    "    final_clf = SubspaceRulesClassifier(rules=rules, max_depth=MAX_DEPTH, random_state=42)\n",
    "    final_clf.fit(x_train, y_train)\n",
    "    final_clf_y_pred = final_clf.predict(x_test)\n",
    "    final_model_test_accuracy = accuracy_score(y_test, final_clf_y_pred) \n",
    "    final_model_confusion_matrix = confusion_matrix(y_test, final_clf_y_pred)\n",
    "    \n",
    "    scores = {\n",
    "        **scores,\n",
    "        **dict(additional_scores),\n",
    "    }\n",
    "    scores_without_test_preffix = {\n",
    "        **{k[5:] if k.startswith('test_') else k: np.mean(v) for k, v in scores.items()},\n",
    "        **{f\"{k[5:]}_stddev\" if k.startswith('test_') else f\"{k}_stddev\": np.std(v) for k, v in scores.items()},\n",
    "    }\n",
    "    \n",
    "    score_by_selection_method = {\n",
    "        'score ' + method: float(parse_expr(method).evalf(subs=scores_without_test_preffix)) for method in SELECTION_METHODS\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        **scores_without_test_preffix, \n",
    "        **score_by_selection_method,\n",
    "        'final_model_test_accuracy': final_model_test_accuracy,\n",
    "        'final_model_test_predictions': final_clf_y_pred,\n",
    "        'final_model_confusion_matrix': final_model_confusion_matrix,\n",
    "        'final_model_used_trees': len(final_clf._clf_by_rule),\n",
    "        'final_model_test_f1_score': f1_score(y_test, final_clf_y_pred, average='weighted'),\n",
    "        'final_model_test_g_mean': geometric_mean_score(y_test, final_clf_y_pred, average='weighted'),\n",
    "        'final_model_test_recall_score': recall_score(y_test, final_clf_y_pred, average='weighted'),\n",
    "        'final_model_test_precision_score': precision_score(y_test, final_clf_y_pred, average='weighted'),\n",
    "        'final_model_test_balanced_accuracy_score': balanced_accuracy_score(y_test, final_clf_y_pred),\n",
    "        'final_model_original_rf_fidelity': accuracy_score(rf_test_predict, final_clf_y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import contextlib\n",
    "# import joblib\n",
    "# from tqdm.notebook import tqdm    \n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# @contextlib.contextmanager\n",
    "# def tqdm_joblib(tqdm_object):\n",
    "#     \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "#     class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "#         def __init__(self, *args, **kwargs):\n",
    "#             super().__init__(*args, **kwargs)\n",
    "\n",
    "#         def __call__(self, *args, **kwargs):\n",
    "#             tqdm_object.update(n=self.batch_size)\n",
    "#             return super().__call__(*args, **kwargs)\n",
    "\n",
    "#     old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "#     joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "#     try:\n",
    "#         yield tqdm_object\n",
    "#     finally:\n",
    "#         joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "#         tqdm_object.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "scoring_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.8 s, sys: 602 ms, total: 8.4 s\n",
      "Wall time: 7.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with joblib.parallel_backend(parallel_backend):\n",
    "#     with tqdm_joblib(tqdm(desc=\"My calculation\", total=len(subspaces_to_check))) as progress_bar:\n",
    "    score_by_subspace = \\\n",
    "        dict(zip(\n",
    "            map(tuple)(subspaces_to_check), \n",
    "            Parallel(n_jobs=N_JOBS)(delayed(lambda subspace: get_val(subspace, x_train, y_train, x_test, y_test))(subspace) for subspace in subspaces_to_check)\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_by_subspace_as_list = list(score_by_subspace.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized_results = np.array([\n",
    "#     [\n",
    "#         scores['balanced_accuracy'],\n",
    "#         scores['f1'],\n",
    "#         scores['accuracy'],\n",
    "#         scores['g_mean'],\n",
    "#         scores['recall'],\n",
    "#         scores['precision'],\n",
    "#         scores['a'],\n",
    "#         scores['b'],\n",
    "#         scores['c'],\n",
    "#         scores['d'],\n",
    "#         scores['s'],\n",
    "#         scores['n']\n",
    "        \n",
    "#     ] for scores in score_by_subspace.values()\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def closest_node(node, nodes):\n",
    "#     nodes = np.asarray(nodes)\n",
    "#     dist_2 = np.sum((nodes - node)**2, axis=1)\n",
    "#     return np.argmin(dist_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closest_node(np.array(current_value), vectorized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_value = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 5, 5, 10, 10, 0.5, 0.5]\n",
    "# for rules, scores in score_by_subspace.items():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from pymoo.model.problem import Problem\n",
    "\n",
    "# class MyProblem(Problem):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__(n_var=12,\n",
    "#                          n_obj=4,\n",
    "#                          n_constr=0,\n",
    "#                          xl=np.array([0,0,0,0,0,0,0,0,0,0,0,0]),\n",
    "#                          xu=np.array([1,1,1,1,1,1, len(x_train), len(x_train), len(x_train), len(x_train), 1, 1]))\n",
    "\n",
    "#     def _evaluate(self, X, out, *args, **kwargs):\n",
    "#         closest_points = [\n",
    "#             closest_node(x, vectorized_results) for x in X\n",
    "#         ]\n",
    "        \n",
    "#         f1 = np.array([\n",
    "#             1 - score_by_subspace_as_list[point][1]['recall'] for point in closest_points\n",
    "#         ])\n",
    "#         f2 = np.array([\n",
    "#             1 - score_by_subspace_as_list[point][1]['precision'] for point in closest_points\n",
    "#         ])\n",
    "#         f3 = np.array([\n",
    "#             1 - score_by_subspace_as_list[point][1]['g_mean'] for point in closest_points\n",
    "#         ])\n",
    "#         f4 = np.array([\n",
    "#             1 - score_by_subspace_as_list[point][1]['balanced_accuracy'] for point in closest_points\n",
    "#         ])\n",
    "        \n",
    "#         out[\"F\"] = np.column_stack([f1, f2, f3, f4])\n",
    "# #         out[\"G\"] = np.column_stack([g1, g2])\n",
    "\n",
    "\n",
    "# vectorized_problem = MyProblem()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymoo.algorithms.nsga2 import NSGA2\n",
    "# from pymoo.factory import get_sampling, get_crossover, get_mutation\n",
    "# from pymoo.factory import get_termination\n",
    "# from pymoo.optimize import minimize\n",
    "\n",
    "# algorithm = NSGA2(\n",
    "#     pop_size=100,\n",
    "#     n_offsprings=20,\n",
    "#     sampling=get_sampling(\"real_random\"),\n",
    "#     crossover=get_crossover(\"real_sbx\", prob=0.9, eta=15),\n",
    "#     mutation=get_mutation(\"real_pm\", eta=20),\n",
    "#     eliminate_duplicates=True\n",
    "# )\n",
    "\n",
    "\n",
    "# termination = get_termination(\"n_gen\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = minimize(vectorized_problem,\n",
    "#                algorithm,\n",
    "#                termination,\n",
    "#                seed=1,\n",
    "#                save_history=True,\n",
    "#                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scoring_start = time.time()\n",
    "# score_by_subspace = {\n",
    "#     tuple(subspace): get_val(subspace, x_train, y_train, x_test, y_test) for subspace in tqdm(subspaces_to_check)\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_end = time.time()\n",
    "scoring_time = scoring_end - scoring_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rules = max(score_by_subspace, key=lambda s: score_by_subspace[s]['final_model_test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = score_by_subspace[top_rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_selection_method = {}\n",
    "for selection_method in SELECTION_METHODS:\n",
    "    best_score = max([s[f'score {selection_method}'] for s in score_by_subspace.values()])\n",
    "    best_score_rules = [rules for rules, val in score_by_subspace.items() if val[f'score {selection_method}'] == best_score]\n",
    "    \n",
    "    best_score_rules_with_scoring = {\n",
    "        rules: score_by_subspace[rules] for rules in best_score_rules\n",
    "    } \n",
    "    worst = best_score_rules_with_scoring[min(best_score_rules_with_scoring, key=lambda v: best_score_rules_with_scoring[v]['accuracy'])]\n",
    "    best = best_score_rules_with_scoring[max(best_score_rules_with_scoring, key=lambda v: best_score_rules_with_scoring[v]['accuracy'])]\n",
    "    \n",
    "    scores_by_selection_method[selection_method] = {\n",
    "        **{f'worst_{k}': v for k, v in worst.items()},\n",
    "        **{f'best_{k}': v for k, v in worst.items()},\n",
    "        'found': len(best_score_rules)\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    **meta,\n",
    "    'scoring_time': scoring_time,\n",
    "    **scores_by_selection_method,\n",
    "    **{f'top_{k}': v for k, v in top.items()},\n",
    "    'found_rules': len(best_score_rules_with_scoring),\n",
    "    'all_subspaces_with_score': score_by_subspace\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARALLELISM == 'dask':\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'workdir/results/res1-new.results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-83eb9b2ed2aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_RESULTS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'workdir/results/res1-new.results'"
     ]
    }
   ],
   "source": [
    "with open(SAVE_RESULTS_PATH, 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib.externals.loky import get_reusable_executor\n",
    "get_reusable_executor().shutdown(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
